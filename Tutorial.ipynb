{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for \"learning electronic metamaterial physics with chatgpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "# create prompt\n",
    "def trans_to_llm_data(mode,data_size):\n",
    "\n",
    "    ## create prompt\n",
    "    file_path = f'/home/dl370/data/LLM/150_350THz_dataset/g_{mode}_150_350THz_f50_d{data_size}.csv'\n",
    "    data_x = pd.read_csv(file_path,header=None)\n",
    "    \n",
    "    # vector\n",
    "    formatted_questions7 = []\n",
    "\n",
    "    for index, row in data_x.iterrows():\n",
    "        question = (\n",
    "            f\" The All-dielectric metasurface suspend in free space is: <{round(float(row.iloc[0]), 3)}, {round(float(row.iloc[1]), 3)}, {round(float(row.iloc[2]),3)}, {round(float(row.iloc[3]),3)},\"\n",
    "            f\" {round(float(row.iloc[4]), 3)}, {round(float(row.iloc[5]), 3)}, {round(float(row.iloc[6]),3)}, {round(float(row.iloc[7]),3)},\"\n",
    "            f\" {round(float(row.iloc[8]), 3)}, {round(float(row.iloc[9]), 3)}, {round(float(row.iloc[10]),3)}, {round(float(row.iloc[11]),3)},\"\n",
    "            f\" {round(float(row.iloc[12]), 3)}, {round(float(row.iloc[13]), 3)}> Get the absorptivity ###\"\n",
    "        )\n",
    "        formatted_questions7.append(question)\n",
    "\n",
    "    ## create completion\n",
    "    file_pathy = f'/home/dl370/data/LLM/150_350THz_dataset/s_{mode}_150_350THz_f50_d{data_size}.csv'\n",
    "    data_y = pd.read_csv(file_pathy,header=None)\n",
    "\n",
    "    formatted_answers = []\n",
    "    for index, row in data_y.iterrows():\n",
    "        answer = np.array(row)\n",
    "        # Round each element to 3 decimal places\n",
    "        rounded_answer = np.around(answer, decimals=3)\n",
    "        formatted_answers.append(rounded_answer ) \n",
    "    print(len((formatted_answers[1])))\n",
    "\n",
    "\n",
    "    option = [7]\n",
    "    questions = [ formatted_questions7]\n",
    "    for index2 , (option, question) in enumerate(zip(option, questions)):\n",
    "        pairs = zip(question, formatted_answers)\n",
    "\n",
    "        output_file_path = f'data_{mode}_{data_size}_1535_output50.jsonl'\n",
    "        print(output_file_path)\n",
    "\n",
    "        system_input = ''\n",
    "\n",
    "        # Writing to the JSONL file\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            \n",
    "            for index, (question, answer) in enumerate(pairs):\n",
    "                question_str = str(question)\n",
    "                answer_str=' ['\n",
    "                number=' '.join(map(lambda x: f\"{x}\", answer))\n",
    "                answer_str += number\n",
    "                answer_str +=']@@@'\n",
    "\n",
    "                # Create a JSON object for each pair\n",
    "                formatted_question = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_input\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": question_str\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": answer_str\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                # Convert the dictionary to a JSON string\n",
    "                json_object = json.dumps(formatted_question)\n",
    "                file.write(json_object + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "data_train_100_1535_output50.jsonl\n",
      "50\n",
      "data_test_10_1535_output50.jsonl\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "mode = 'train'\n",
    "data_size = '100'\n",
    "trans_to_llm_data(mode,data_size)\n",
    "modey = 'test'\n",
    "datay_size = '10'\n",
    "trans_to_llm_data(modey,datay_size)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Chatgpt and Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# openai setting\n",
    "api_key =\"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\"\n",
    "openai.api_key = api_key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_create(mode,size):\n",
    "  # call openai api to create fine-tune\n",
    "  file2 =  client.files.create(\n",
    "  file=open(f\"data_{mode}_{size}_1535_output50.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    "  )\n",
    "\n",
    "  client.fine_tuning.jobs.create(\n",
    "  training_file=file2.id, \n",
    "  validation_file = None,\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  suffix = f\"f{size}2_1535\",\n",
    "  hyperparameters={\n",
    "  \"n_epochs\": 9,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may takes a while to finish the fine-tune\n",
    "fine_tune_create(mode,data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import json\n",
    "def validate_number_format(number):\n",
    "    # as the absorptivity should be in [0,1]\n",
    "    if not '0.' in number:\n",
    "        if not float(number)==0:\n",
    "            print(number)\n",
    "            raise ValueError(f\"Invalid number format: '{number}' contains spaces instead of being formatted as '0.xxx'.\")\n",
    "        \n",
    "def get_conversation(size):\n",
    "  test_file_path = f'data_test_{size}_1535_output50.jsonl'\n",
    "\n",
    "  test_data = []\n",
    "\n",
    "  with open(test_file_path, 'r') as file:\n",
    "      for line in file:\n",
    "          json_object = json.loads(line)\n",
    "          # Check if the JSON object has the key \"messages\"\n",
    "          if \"messages\" in json_object:\n",
    "              test_data.append(json_object[\"messages\"])\n",
    "  print(len(test_data))\n",
    "  user_system_messages = []\n",
    "  assistant_responses = []\n",
    "\n",
    "  # Iterate over each entry in test_data\n",
    "  for conversation in test_data:\n",
    "      current_conversation = []\n",
    "      for message in conversation:\n",
    "          if message['role'] == 'assistant':\n",
    "              assistant_responses.append(np.array(message['content']))\n",
    "          else:\n",
    "              current_conversation.append(message)\n",
    "      user_system_messages.append(current_conversation)\n",
    "  return user_system_messages, assistant_responses\n",
    "\n",
    "def process_res(resp):\n",
    "\n",
    "    # Removing non-numeric characters and splitting into individual elements\n",
    "\n",
    "    cleaned_data = resp.tolist().split('[')[1].split(']')[0]\n",
    "    number_strings = cleaned_data.split(' ')\n",
    "    number_list = []\n",
    "    for num in number_strings:\n",
    "        if num != '':\n",
    "            validate_number_format(num.replace(',', ' '))\n",
    "            number_list.append(float(num.replace(',', ' ')))\n",
    "    return number_list\n",
    "\n",
    "def mean_absolute_percentage_error2(true_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Percentage Error (MAPE).\n",
    "    \"\"\"\n",
    "    true_values = np.array(true_values)\n",
    "    predicted_values = np.array(predicted_values)\n",
    "\n",
    "    # adding epsilon to avoid division by zero\n",
    "    absolute_percentage_errors = np.abs((true_values - predicted_values) / (true_values+1e-4))\n",
    "    mape = np.mean(absolute_percentage_errors)\n",
    "    return mape\n",
    "\n",
    "def get_response(temperature_set,user_system_messages,test_model,design=0):\n",
    "  rep_all = []\n",
    "  count = 0\n",
    "  for temp in temperature_set:\n",
    "      responses = []\n",
    "\n",
    "      iall=0\n",
    "      for conversation in tqdm(user_system_messages):\n",
    "          iall+=1\n",
    "          it=0\n",
    "          \n",
    "          while True:  # Continue trying until no ValueError\n",
    "              it+=1\n",
    "              response = client.chat.completions.create(\n",
    "                  model=test_model,\n",
    "                  messages=conversation,\n",
    "                  temperature=temp\n",
    "              )\n",
    "              if it > 3 or design == 1:\n",
    "                  responses.append(np.array(response.choices[0].message.content))\n",
    "                  if design == 0:\n",
    "                    print('failed in transforming', response.choices[0].message.content)\n",
    "                  break\n",
    "              try:\n",
    "                  processed_response = process_res(np.array(response.choices[0].message.content))\n",
    "                  responses.append(np.array(response.choices[0].message.content))\n",
    "                  break\n",
    "              except ValueError:\n",
    "                  print('can not trans')\n",
    "                  print(response.choices[0].message.content)\n",
    "                  count += 1\n",
    "                  continue\n",
    "              except IndexError:\n",
    "                  print('can not trans')\n",
    "                  print(response.choices[0].message.content)\n",
    "                  count += 1\n",
    "                  continue \n",
    "      rep_all.append(responses)\n",
    "      print('error count',count)\n",
    "  return rep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error count 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the model name in the OpenAI dashboard to ensure it is correct\n",
    "test_model = 'ft:gpt-3.5-turbo-1106:mlm:f1002-1535:BENP4RgZ'\n",
    "temperature_set=[1]\n",
    "user_system_messages,assistant_responses =  get_conversation(datay_size)\n",
    "rep_all = get_response(temperature_set,user_system_messages,test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_MSE_MARE(rep_all,assistant_responses):\n",
    "  modes=10\n",
    "\n",
    "  mse_all=[]\n",
    "  mare_all=[]\n",
    "  for rep in rep_all:\n",
    "      float_arrays= []\n",
    "      float_assistant_responses = []\n",
    "      for response, truth in zip(rep, assistant_responses):\n",
    "          try:\n",
    "              float_arrays.append(process_res(response))\n",
    "              float_assistant_responses.append(process_res(truth))\n",
    "          except ValueError:\n",
    "              continue\n",
    "          except IndexError:\n",
    "              print(response)\n",
    "              continue\n",
    "\n",
    "      # Calculate MSE for each pair and average them\n",
    "      mse_values = []\n",
    "      mare_values=[]\n",
    "      for arr1, arr2 in zip(float_arrays, float_assistant_responses):\n",
    "          min_len = min(len(arr2), len(arr1))\n",
    "          arr1_padded, arr2_padded = (arr1[:min_len], arr2[:min_len])\n",
    "          mse = mean_squared_error(arr1_padded, arr2_padded)\n",
    "          mare = mean_absolute_percentage_error2(arr1_padded, arr2_padded)\n",
    "          mse_values.append(mse)\n",
    "          mare_values.append(mare)\n",
    "      mse_all.append(mse_values)\n",
    "      mare_all.append(mare_values)\n",
    "  return mse_all,mare_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_all,mare_all = get_MSE_MARE(rep_all,assistant_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg mse [0.03334132]\n",
      "avg mare [0.63501585]\n"
     ]
    }
   ],
   "source": [
    "print('avg mse',np.mean(np.array(mse_all),axis=1))\n",
    "print('avg mare',np.mean(np.array(mare_all),axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
